{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f077c438-88c6-43bf-9908-9a2d3f872bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Date  Flow\n",
      "0    2019-02-01 05:00:00+00:00  5.23\n",
      "1    2019-02-01 05:15:00+00:00  5.23\n",
      "2    2019-02-01 05:30:00+00:00  5.22\n",
      "3    2019-02-01 05:45:00+00:00  5.21\n",
      "4    2019-02-01 06:00:00+00:00  5.22\n",
      "...                        ...   ...\n",
      "5751 2019-04-02 02:45:00+00:00  6.81\n",
      "5752 2019-04-02 03:00:00+00:00  6.78\n",
      "5753 2019-04-02 03:15:00+00:00  6.76\n",
      "5754 2019-04-02 03:30:00+00:00  6.78\n",
      "5755 2019-04-02 03:45:00+00:00  6.76\n",
      "\n",
      "[5756 rows x 2 columns]\n",
      "                          Date    Flow\n",
      "0    2019-02-01 05:00:00+00:00  5.2225\n",
      "1    2019-02-01 06:00:00+00:00  5.2200\n",
      "2    2019-02-01 07:00:00+00:00  5.2225\n",
      "3    2019-02-01 08:00:00+00:00  5.2175\n",
      "4    2019-02-01 09:00:00+00:00  5.2175\n",
      "...                        ...     ...\n",
      "1434 2019-04-01 23:00:00+00:00  6.9100\n",
      "1435 2019-04-02 00:00:00+00:00  6.8000\n",
      "1436 2019-04-02 01:00:00+00:00  6.8825\n",
      "1437 2019-04-02 02:00:00+00:00  6.8325\n",
      "1438 2019-04-02 03:00:00+00:00  6.7700\n",
      "\n",
      "[1439 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code will grab USGS streamflow data for a defined period of interest \n",
    "\n",
    "'''\n",
    "\n",
    "# first import the functions for downloading data from NWIS\n",
    "import dataretrieval.nwis as nwis\n",
    "import pandas as pd\n",
    "\n",
    "# specify the USGS site code for which we want data.\n",
    "site = ['01054500']\n",
    "\n",
    "# get instantaneous values (iv)\n",
    "df = nwis.get_record(sites=site, service='iv', start='2019-02-01', end='2019-04-01')\n",
    "df.reset_index(inplace=True) #reset index to grab station date\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True, format = '%Y-%m-%d %H:%M:%S') #transfer to utc so same time throughout\n",
    "df_quarter = df.iloc[:,[0,4]] #locates every row by the columns we want (date and flow)\n",
    "df_quarter.columns = ['Date', 'Flow'] # rename column headers \n",
    "print(df_quarter)\n",
    "\n",
    "## Make copy od dataframe to average flow every hour \n",
    "df_copy = df.copy()\n",
    "df_copy['datetime'] = pd.to_datetime(df_copy['datetime'], utc=True, format = '%Y-%m-%d %H:%M:%S') #convert datetime to average every hour \n",
    "#df_copy.reset_index(inplace=True) #reset indexes so can grab \"Date\" column\n",
    "#df_copy.drop(\"datetime\", axis=1, inplace=True) #drop unneccsary date column now \n",
    "df_copy.index = df_copy['datetime'] # index so can pull date time in resample\n",
    "df_avg = df_copy.resample('H').mean() # Average every hour based on datetime\n",
    "df_avg.reset_index(inplace=True) #reset index again to have datetime \n",
    "df_avgflow = df_avg.iloc[:,[0,2]] #locates every row by the columns we want (date and flow)\n",
    "df_avgflow.columns = ['Date', 'Flow']\n",
    "print(df_avgflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a37d26a-9601-48d8-b1e6-b210d436cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Date  Flow  Min_Error  Max_Error\n",
      "0    2019-02-01 05:00:00+00:00  5.23     0.1046      1.046\n",
      "1    2019-02-01 05:15:00+00:00  5.23     0.1046      1.046\n",
      "2    2019-02-01 05:30:00+00:00  5.22     0.1044      1.044\n",
      "3    2019-02-01 05:45:00+00:00  5.21     0.1042      1.042\n",
      "4    2019-02-01 06:00:00+00:00  5.22     0.1044      1.044\n",
      "...                        ...   ...        ...        ...\n",
      "5751 2019-04-02 02:45:00+00:00  6.81     0.1362      1.362\n",
      "5752 2019-04-02 03:00:00+00:00  6.78     0.1356      1.356\n",
      "5753 2019-04-02 03:15:00+00:00  6.76     0.1352      1.352\n",
      "5754 2019-04-02 03:30:00+00:00  6.78     0.1356      1.356\n",
      "5755 2019-04-02 03:45:00+00:00  6.76     0.1352      1.352\n",
      "\n",
      "[5756 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "## Make new list of standard error associated with each flow measurement in 15min USGS data\n",
    "df_error = df_quarter.loc[:,\"Flow\"]\n",
    "\n",
    "#calculate minimum error of flow\n",
    "df_error_min = df_error*0.02\n",
    "#calculate maximum error of flow\n",
    "df_error_max = df_error*0.2\n",
    "\n",
    "#concatonate error columns together\n",
    "df_error_table = pd.concat([df_error_min, df_error_max],axis=1)\n",
    "df_error_table.columns = ['Min_Error', 'Max_Error']\n",
    "#print(df_error_table)\n",
    "\n",
    "#concatonate 15min USGS data with error bars\n",
    "df_final = pd.concat([df_quarter, df_error_table], axis=1)\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dbd21-2da7-4cae-b565-d689d60e6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
